{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert chest CT scans from a set of .png images to .nii files\n",
    "# \n",
    "# This should only be done the first time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â The CLEAN-CC-CCII dataset is available at the following URL: https://github.com/HKBU-HPML/HKBU_HPML_COVID-19 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def png_to_nii(in_path, out_path):\n",
    "    paths=[x[0] for x in os.walk(in_path)]\n",
    "\n",
    "    i=0\n",
    "    scan_name=''\n",
    "    for path in paths:\n",
    "        file_names=list()\n",
    "        for File in os.listdir(path):\n",
    "            if File.endswith(\".png\"): \n",
    "                file_names.append(path+os.path.sep+File)\n",
    "        if len(file_names)!=0: \n",
    "            i+=1\n",
    "            l=path[path.rfind(os.path.sep)+1:]\n",
    "            path=path[:path.rfind(os.path.sep)]\n",
    "            f=path[path.rfind(os.path.sep)+1:]\n",
    "            scan_name=f+'_'+l\n",
    "            #file_names = glob.glob('*.png')\n",
    "            try:\n",
    "                reader = sitk.ImageSeriesReader()\n",
    "                reader.SetFileNames(file_names)\n",
    "                vol = reader.Execute()\n",
    "                sitk.WriteImage(vol, os.path.join(out_path, scan_name+\".nii.gz\")\n",
    "            except: print (scan_name)\n",
    "\n",
    "# replace with your directories with the paths of the CLEAN-CC-CCII dataset and the path where save the nii files\n",
    "                                \n",
    "in_paths=[\"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/Normal/\",\n",
    "          \"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/NCP/\",\n",
    "          \"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/CP/\"\n",
    "         ]\n",
    "out_paths=[\"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/nii/Normal/\",\n",
    "           \"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/nii/NCP/\",\n",
    "           \"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/nii/CP/\"\n",
    "          ]\n",
    "\n",
    "for i in range(len(in_paths)):\n",
    "    png_to_nii(in_path[i], out_path[i])                               \n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the chest CT scans (.nii files) with the aim to resize them and remove irrelevant contents\n",
    "# The following will generate the training/validation/test set for a k-fold cross validation strategy according\n",
    "# to the required CT scan depth.\n",
    "# Datasets will be saved as numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from numpy.random import default_rng\n",
    "import nibabel as nib\n",
    "from scipy import ndimage\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "import multiprocessing as mp\n",
    "from scipy.ndimage import zoom\n",
    "import tensorflow as tf\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the parameters to resize the chest CT scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_width=256   # width of the slices after pre-processing\n",
    "new_height=256  # height for the slices after pre-processing\n",
    "new_depth=25    # number of slices of the CT after pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pre-processing steps for the CT scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = nib.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "# upsample the scan at borders\n",
    "def upsample_borders(img, current_depth, new_depth):\n",
    "    \"\"\"Upsample scans by repeating slices at both extremity\"\"\"\n",
    "    up = new_depth - current_depth\n",
    "    upl = round(up/2)\n",
    "    upr = new_depth - (current_depth + upl)\n",
    "    idx_l = [0] * upl\n",
    "    idx_r = [img.shape[2]-1] * upr\n",
    "    slices=np.array(range(current_depth))\n",
    "    slices=np.hstack((slices, np.array(idx_r, dtype=int), np.array(idx_l, dtype=int)))\n",
    "    slices.sort(axis=0)\n",
    "    slices=np.asarray(slices)\n",
    "    img=img[:,:,slices]\n",
    "    \n",
    "    return img\n",
    "\n",
    "# downsample the scan at borders\n",
    "def downsample_borders(img, current_depth, new_depth):\n",
    "    \"\"\"Downsample scans by repeating slices at both extremity\"\"\"\n",
    "    down = current_depth - new_depth\n",
    "    downl = round(down/2)\n",
    "    downr = current_depth - (new_depth + downl)\n",
    "    slices = np.array(range(current_depth))\n",
    "    slices = slices[downl:current_depth-downr]\n",
    "    img=img[:,:,slices]\n",
    "    \n",
    "    return img\n",
    "\n",
    "\n",
    "# crop and scale the slices of a CT scan\n",
    "def resize_volume(img, width, height, depth):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    new_depth = depth\n",
    "    new_width = width\n",
    "    new_height = height\n",
    "    \n",
    "    # Get current depth\n",
    "    current_depth = img.shape[2]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    \n",
    "    # Upsampling/Downsampling slices at both extremity\n",
    "    if new_depth<current_depth: img = downsample_borders(img, current_depth, new_depth)\n",
    "    elif new_depth>current_depth: img = upsample_borders(img, current_depth, new_depth)\n",
    "    else: pass # depth will not be changed => desired_depth==current_depth\n",
    "    \n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, -90, reshape=False)\n",
    "    #img = ndimage.rotate(img, 180, reshape=False)\n",
    "    #img = np.flipud(img)\n",
    "    \n",
    "    crop_scan = img[img.shape[0]//6: - img.shape[0]//6, img.shape[1]//6: - img.shape[1]//6, :]\n",
    "    depth_factor = 1.0 # only used for zooming. Images were previously upsampled/downsampled \n",
    "    width_factor = 1/(crop_scan.shape[0] / new_width)\n",
    "    height_factor = 1/(crop_scan.shape[1] / new_height)\n",
    "    img= ndimage.zoom(crop_scan, (width_factor, height_factor, depth_factor))\n",
    "\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "def process_scan(path):\n",
    "    width=new_width\n",
    "    height=new_height\n",
    "    depth=new_depth\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    if len(volume.shape)==5: volume=volume[:,:,:,0,0]\n",
    "    # Normalize\n",
    "    volume = volume.astype(\"float32\")\n",
    "    # Resize width, height and depth\n",
    "    volume = resize_volume(volume, width, height, depth)\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal scans: 966\n",
      "NCP scans: 1519\n",
      "CP scans: 1538\n",
      "Pre-processing normal scans\n",
      "Samples classified as Normal:  966\n",
      "Pre-processing NCP scans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/wmlce/lib/python3.7/site-packages/scipy/ndimage/interpolation.py:611: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples classified as NCP:  1519\n",
      "Pre-processing CP scans\n",
      "Samples classified as CP :  1538\n",
      "(1, 966, 256, 256, 25)\n",
      "(1, 1519, 256, 256, 25)\n",
      "(1, 1538, 256, 256, 25)\n"
     ]
    }
   ],
   "source": [
    "def pre_processing(normal_scans_path, ncp_scans_path, cp_scans_path):\n",
    "\n",
    "    # Read and process the scans.\n",
    "    pool = mp.Pool(mp.cpu_count())\n",
    "    \n",
    "    print('Pre-processing CT scans of Normal controls')\n",
    "    normal_scans = np.array([pool.map(process_scan, normal_scans_path)])\n",
    "    print('Samples classified as Normal: ', normal_scans.shape[1])\n",
    "    \n",
    "    print('Pre-processing CT scans of patients with NCP')\n",
    "    ncp_scans = np.array([pool.map(process_scan, ncp_scans_path)])\n",
    "    print('Samples classified as NCP: ', ncp_scans.shape[1])\n",
    "    \n",
    "    print('Pre-processing CT scans of patients with CP')\n",
    "    cp_scans = np.array([pool.map(process_scan, cp_scans_path)])\n",
    "    print('Samples classified as CP : ', cp_scans.shape[1])\n",
    "    \n",
    "    pool.close()\n",
    "      \n",
    "    # labels for training set\n",
    "    # NORMAL\n",
    "    normal_labels_indexes = np.array([0 for _ in range(len(normal_scans_path))])\n",
    "    # NCP\n",
    "    ncp_labels_indexes = np.array([1 for _ in range(len(ncp_scans_path))])\n",
    "    # CP\n",
    "    cp_labels_indexes = np.array([2 for _ in range(len(cp_scans_path))])\n",
    "    \n",
    "    nb_classes=3\n",
    "    normal_labels=tf.one_hot(normal_labels_indexes, nb_classes) \n",
    "    ncp_labels=tf.one_hot(ncp_labels_indexes, nb_classes) \n",
    "    cp_labels=tf.one_hot(cp_labels_indexes, nb_classes) \n",
    "    \n",
    "    normal_labels=tf.make_tensor_proto(normal_labels)\n",
    "    ncp_labels=tf.make_tensor_proto(ncp_labels)\n",
    "    cp_labels=tf.make_tensor_proto(cp_labels)\n",
    "    \n",
    "    normal_labels = tf.make_ndarray(normal_labels)\n",
    "    ncp_labels = tf.make_ndarray(ncp_labels)\n",
    "    cp_labels = tf.make_ndarray(cp_labels)\n",
    "    \n",
    "    print(normal_scans.shape)\n",
    "    print(ncp_scans.shape)\n",
    "    print(cp_scans.shape)\n",
    "    \n",
    "    x = np.hstack((normal_scans,\n",
    "                   ncp_scans,\n",
    "                   cp_scans\n",
    "                  )) \n",
    "    y = np.concatenate((normal_labels, \n",
    "                   ncp_labels,\n",
    "                   cp_labels\n",
    "                  ))\n",
    "    \n",
    "    #print(x.shape)\n",
    "    #print(y.shape)\n",
    "    x = x.squeeze()\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "# get chest CT scan paths (saved as .nii)\n",
    "\n",
    "# replace the scans_path variable with your directories\n",
    "\n",
    "# get paths of the CT scans of normal controls\n",
    "scans_path=\"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/nii/Normal\" # folder with scans of normal controls\n",
    "normal_scans_paths = [\n",
    "    os.path.join(os.getcwd(), scans_path, x)\n",
    "    for x in os.listdir(scans_path)\n",
    "]\n",
    "\n",
    "# get paths of the CT scans of patients affected by NCP\n",
    "scans_path=\"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/nii/NCP\" # folder with scans of patients affected by NCP\n",
    "ncp_scans_paths = [\n",
    "    os.path.join(os.getcwd(), scans_path, x)\n",
    "    for x in os.listdir(scans_path)\n",
    "]\n",
    "\n",
    "# get paths of the CT scans of patients affected by NCP\n",
    "scans_path=\"/home/pwrai/notebook/Clean-CC-CCII/dataset_cleaned/nii/CP\" # folder with scans of patients affected by CP\n",
    "cp_scans_paths = [\n",
    "    os.path.join(os.getcwd(), scans_path, x)\n",
    "    for x in os.listdir(scans_path)\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Normal scans: \" + str(len(normal_scans_paths)))\n",
    "print(\"NCP scans: \" + str(len(ncp_scans_paths)))\n",
    "print(\"CP scans: \" + str(len(cp_scans_paths)))\n",
    "\n",
    "X, Y = pre_processing(normal_scans_paths, ncp_scans_paths, cp_scans_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and save training, validation and test set for all folds (to implement a k-fold cross validation strategy)\n",
    "# by default a 5-fold cross validation is implemented which implies that for each fold the dataset is split into \n",
    "# 20% for test set and 80% for training set. We also imposed that 10% of the training set was used for validation\n",
    "# purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from skmultilearn.model_selection import IterativeStratification\n",
    "from os import path\n",
    "\n",
    "n_splits=5 # nb of folds\n",
    "kfold = IterativeStratification(n_splits=n_splits, order=1)\n",
    "\n",
    "i = 0\n",
    "\n",
    "# replace \"folds_path\" with your directory\n",
    "# for each fold a directory labeled fold_i (i stands the i-th fold) will be automatically created\n",
    "folds_path=\"/home/pwrai/notebook/clean_cc_ccii_folds/25/\"\n",
    "\n",
    "for train, test in kfold.split(X, Y):\n",
    "    i+=1\n",
    "    x_train, x_val, y_train, y_val = train_test_split(X[train], Y[train], test_size=0.1,  stratify=Y[train])\n",
    "    x_test = X[test]\n",
    "    y_test = Y[test]\n",
    "    \n",
    "    fold_path=path.join(folds_path, \"fold_\"+str(i) + path.sep)\n",
    "        \n",
    "    Path(fold_path).mkdir(parents=True, exist_ok=True)\n",
    "    np.save(fold_path+'x_train.npy', x_train)\n",
    "    np.save(fold_path+'x_test.npy', x_test)\n",
    "    np.save(fold_path+'x_val.npy', x_val)\n",
    "    np.save(fold_path+'y_train.npy', y_train)\n",
    "    np.save(fold_path+'y_test.npy', y_test)\n",
    "    np.save(fold_path+'y_val.npy', y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
